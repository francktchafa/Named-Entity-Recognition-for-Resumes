{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:35.286997Z",
     "start_time": "2025-05-05T19:53:30.162735Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Transformer Network Application: Resume Named-Entity Recognition (NER)\n",
    "\"\"\"\n",
    "from utils_ner import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import DistilBertTokenizerFast \n",
    "from transformers import TFDistilBertForTokenClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Set TensorFlow logger to only display ERROR messages\n",
    "tf.get_logger().setLevel('ERROR')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:38.872001Z",
     "start_time": "2025-05-05T19:53:38.869816Z"
    }
   },
   "source": [
    "# GPU SETTINGS\n",
    "# TensorFlow configuration to efficiently manages GPU memory by restricting \n",
    "# the default behavior of allocating all available GPU memory, ensuring fair\n",
    "# resource sharing and avoiding out-of-memory (OOM) errors. \n",
    "\n",
    "configure_gpu_memory(memory_limit=4096, config_option=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found. Configuration skipped.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# DATA CLEANING\n",
    "# A look at the data\n",
    "ner_json_path = os.path.join(os.getcwd(), \"data\", \"ner.json\")\n",
    "df_data = pd.read_json(ner_json_path, lines=True) # 'lines=True' Each line is treated as an independent JSON object.\n",
    "df_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:39.787164Z",
     "start_time": "2025-05-05T19:53:39.751629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha\\nApplication Development Associat...   \n",
       "1  Afreen Jamadar\\nActive member of IIIT Committe...   \n",
       "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...   \n",
       "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...   \n",
       "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...   \n",
       "\n",
       "                                          annotation  extras  \n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...     NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:41.864541Z",
     "start_time": "2025-05-05T19:53:41.861251Z"
    }
   },
   "source": [
    "df_data = df_data.drop(['extras'], axis=1)\n",
    "df_data['content'] = df_data['content'].str.replace(\"\\n\", \" \")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "df_data.iloc[0]['annotation']"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:42.822596Z",
     "start_time": "2025-05-05T19:53:42.817398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': ['Skills'],\n",
       "  'points': [{'start': 1295,\n",
       "    'end': 1621,\n",
       "    'text': '\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player'}]},\n",
       " {'label': ['Skills'],\n",
       "  'points': [{'start': 993,\n",
       "    'end': 1153,\n",
       "    'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]},\n",
       " {'label': ['Graduation Year'],\n",
       "  'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 771,\n",
       "    'end': 813,\n",
       "    'text': 'B.v.b college of engineering and technology'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 727,\n",
       "    'end': 769,\n",
       "    'text': 'B.E in Information science and engineering\\n'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 407, 'end': 415, 'text': 'Accenture'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 372,\n",
       "    'end': 404,\n",
       "    'text': 'Application Development Associate'}]},\n",
       " {'label': ['Email Address'],\n",
       "  'points': [{'start': 95,\n",
       "    'end': 145,\n",
       "    'text': 'Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n'}]},\n",
       " {'label': ['Location'],\n",
       "  'points': [{'start': 60, 'end': 68, 'text': 'Bengaluru'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 49, 'end': 57, 'text': 'Accenture'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 13,\n",
       "    'end': 45,\n",
       "    'text': 'Application Development Associate'}]},\n",
       " {'label': ['Name'],\n",
       "  'points': [{'start': 0, 'end': 11, 'text': 'Abhishek Jha'}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:45.235295Z",
     "start_time": "2025-05-05T19:53:45.200173Z"
    }
   },
   "source": [
    "df_data['entities'] = get_entities(df_data)\n",
    "df_data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha Application Development Associate...   \n",
       "1  Afreen Jamadar Active member of IIIT Committee...   \n",
       "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "\n",
       "                                          annotation  \\\n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...   \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...   \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...   \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...   \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...   \n",
       "\n",
       "                                            entities  \n",
       "0  [(0, 12, Name), (13, 46, Designation), (49, 58...  \n",
       "1  [(0, 14, Name), (62, 68, Location), (104, 148,...  \n",
       "2  [(0, 21, Name), (22, 31, Location), (65, 117, ...  \n",
       "3  [(0, 12, Name), (13, 51, Designation), (54, 60...  \n",
       "4  [(0, 13, Name), (14, 22, Designation), (24, 41...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>[(0, 12, Name), (13, 46, Designation), (49, 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>[(0, 14, Name), (62, 68, Location), (104, 148,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>[(0, 21, Name), (22, 31, Location), (65, 117, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>[(0, 12, Name), (13, 51, Designation), (54, 60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>[(0, 13, Name), (14, 22, Designation), (24, 41...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:46.456605Z",
     "start_time": "2025-05-05T19:53:46.452581Z"
    }
   },
   "source": [
    "# Convert DataTurks to SpaCy for training\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    \"\"\"\n",
    "    Converts Dataturks JSON data into a format compatible with SpaCy training.\n",
    "    \n",
    "    This function processes annotated data from a Dataturks JSON file and transforms it \n",
    "    into a SpaCy-compatible format for Named Entity Recognition (NER) training. The output \n",
    "    includes a list of tuples, where each tuple contains a text string and its corresponding \n",
    "    entity annotations.\n",
    "\n",
    "    Arguments:\n",
    "        dataturks_JSON_FilePath (str): The file path to the Dataturks JSON file. This file \n",
    "                                       should contain annotations for training in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple consists of:\n",
    "              - text (str): The annotated text.\n",
    "              - dict: A dictionary with a key \"entities\", which maps to a list of tuples.\n",
    "                      Each tuple represents an entity and includes:\n",
    "                        - start (int): Start index of the entity in the text.\n",
    "                        - end (int): End index of the entity in the text (exclusive).\n",
    "                        - label (str): The label/category of the entity.\n",
    "\n",
    "    Exception Handling:\n",
    "        In case of any error during file reading or processing, logs the exception \n",
    "        and returns None.\n",
    "\n",
    "    Example Output:\n",
    "        [\n",
    "            (\"John works at Microsoft.\", {\"entities\": [(0, 4, \"PERSON\"), (14, 23, \"ORGANIZATION\")]}),\n",
    "            (\"Paris is beautiful.\", {\"entities\": [(0, 5, \"LOCATION\")]}),\n",
    "            (\"Jacob lives in New York.\", {\"entities\": [(0, 4, \"PERSON\"), (14, 22, \"LOCATION\")]})\n",
    "        ]\n",
    "\n",
    "    Steps:\n",
    "        1. Open the JSON file and read each line.\n",
    "        2. Parse the JSON content to extract text and annotations.\n",
    "        3. Process each annotation to extract the entity label, start, and end positions.\n",
    "        4. Handle whitespace adjustments using `lstrip()` and `rstrip()` to ensure accurate indexing.\n",
    "        5. Append the processed text and entity annotations to the training data list.\n",
    "        6. Return the final training data list in SpaCy format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_data = []  # List to store processed training data\n",
    "        lines = []\n",
    "\n",
    "        # Step 1: Read lines from the Dataturks JSON file\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Step 2: Process each line in the JSON file\n",
    "        for line in lines:\n",
    "            data = json.loads(line)  # Parse the JSON line\n",
    "            text = data['content'].replace(\"\\n\", \" \")  # Replace newlines with spaces in the text\n",
    "            entities = []  # List to store entities for the current text\n",
    "            data_annotations = data['annotation']\n",
    "\n",
    "            # Step 3: Process annotations if they exist\n",
    "            if data_annotations is not None:\n",
    "                for annotation in data_annotations:\n",
    "                    # Process a single point in the text annotation\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "\n",
    "                    # Handle both a single label or a list of labels\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "\n",
    "                        # Adjust start and end indices for leading/trailing whitespace\n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start = point_start + lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end = point_end - rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1, label))  # Adjusted end is exclusive\n",
    "            \n",
    "            # Step 4: Add text and entities to training data\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "        # Step 5: Return the final training data\n",
    "        return training_data\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the exception and return None\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "data_spacy = convert_dataturks_to_spacy(ner_json_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:49.985571Z",
     "start_time": "2025-05-05T19:53:49.970929Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "pd.DataFrame(data_spacy).head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:53:50.668100Z",
     "start_time": "2025-05-05T19:53:50.657760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   0  \\\n",
       "0  Abhishek Jha Application Development Associate...   \n",
       "1  Afreen Jamadar Active member of IIIT Committee...   \n",
       "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "\n",
       "                                                   1  \n",
       "0  {'entities': [(1296, 1622, 'Skills'), (993, 11...  \n",
       "1  {'entities': [(1155, 1199, 'Email Address'), (...  \n",
       "2  {'entities': [(3749, 3757, 'Skills'), (3709, 3...  \n",
       "3  {'entities': [(8098, 8384, 'Skills'), (8008, 8...  \n",
       "4  {'entities': [(2010, 2013, 'Degree'), (973, 17...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>{'entities': [(1296, 1622, 'Skills'), (993, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>{'entities': [(1155, 1199, 'Email Address'), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>{'entities': [(3749, 3757, 'Skills'), (3709, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>{'entities': [(8098, 8384, 'Skills'), (8008, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>{'entities': [(2010, 2013, 'Degree'), (973, 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# Cleans entity spans to remove leading and trailing whitespace\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"\n",
    "    Cleans entity spans by removing leading and trailing whitespace in the annotated text.\n",
    "\n",
    "    This function processes a list of annotated text data in spaCy JSON format. For each entity span,\n",
    "    it ensures that any leading or trailing whitespace in the text does not affect the start and end\n",
    "    positions of the entity annotations. The adjusted spans are then added back to the cleaned data.\n",
    "\n",
    "    Arguments:\n",
    "        data (list): A list of annotated text data in spaCy JSON format. Each element should be a tuple\n",
    "                     containing:\n",
    "                     - text (str): The annotated text.\n",
    "                     - annotations (dict): A dictionary with key \"entities\", mapping to a list of entity spans.\n",
    "                       Each entity span should be a tuple of (start, end, label).\n",
    "\n",
    "    Returns:\n",
    "        list: A cleaned list of annotated text data in spaCy JSON format. Each element is a tuple containing:\n",
    "              - text (str): The original text.\n",
    "              - annotations (dict): A dictionary with key \"entities\", mapping to a list of adjusted entity spans.\n",
    "                Each adjusted entity span is a list with:\n",
    "                  - valid_start (int): The corrected start index of the entity in the text.\n",
    "                  - valid_end (int): The corrected end index of the entity in the text (exclusive).\n",
    "                  - label (str): The entity label.\n",
    "\n",
    "    Example Input:\n",
    "        data = [(\"  John Doe is a developer. \", {\"entities\": [(2, 10, \"PERSON\")]})]\n",
    "\n",
    "    Example Output:\n",
    "        [(\"  John Doe is a developer. \", {\"entities\": [(2, 9, \"PERSON\")]})]\n",
    "\n",
    "    Functionality:\n",
    "        1. Compiles a regex pattern (`r'\\s'`) to match whitespace characters.\n",
    "        2. Iterates over each text and its associated annotations in the input data.\n",
    "        3. For each entity, adjusts the `start` and `end` positions to exclude leading and trailing whitespace.\n",
    "        4. Appends the corrected entities and text back to the cleaned data list.\n",
    "        5. Returns the cleaned data.\n",
    "\n",
    "    Note:\n",
    "        This function ensures that whitespace does not interfere with entity span indices, which is\n",
    "        critical for accurate training in spaCy-based Named Entity Recognition (NER) pipelines.\n",
    "    \"\"\"\n",
    "\n",
    "    invalid_span_tokens = re.compile(r\"\\s\")  # Pattern to identify whitespace\n",
    "\n",
    "    cleaned_data = []  # List to store the cleaned data\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']  # Extract entities for the current text\n",
    "        valid_entities = []  # List to store valid (corrected) entities\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            \n",
    "            # Adjust the start position to skip leading whitespace\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
    "                valid_start += 1\n",
    "            # Adjust the end position to skip trailing whitespace\n",
    "            while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            # Append the corrected entity\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        # Append the cleaned text and entities to the result\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:07.674518Z",
     "start_time": "2025-05-05T19:55:07.670623Z"
    }
   },
   "cell_type": "code",
   "source": "data = trim_entity_spans(data_spacy)",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:11.317793Z",
     "start_time": "2025-05-05T19:55:11.307011Z"
    }
   },
   "source": [
    "pd.DataFrame(data).head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   0  \\\n",
       "0  Abhishek Jha Application Development Associate...   \n",
       "1  Afreen Jamadar Active member of IIIT Committee...   \n",
       "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "\n",
       "                                                   1  \n",
       "0  {'entities': [[1296, 1622, 'Skills'], [993, 11...  \n",
       "1  {'entities': [[1155, 1199, 'Email Address'], [...  \n",
       "2  {'entities': [[3749, 3757, 'Skills'], [3709, 3...  \n",
       "3  {'entities': [[8098, 8384, 'Skills'], [8008, 8...  \n",
       "4  {'entities': [[2010, 2013, 'Degree'], [973, 17...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>{'entities': [[1296, 1622, 'Skills'], [993, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>{'entities': [[1155, 1199, 'Email Address'], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>{'entities': [[3749, 3757, 'Skills'], [3709, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>{'entities': [[8098, 8384, 'Skills'], [8008, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>{'entities': [[2010, 2013, 'Degree'], [973, 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Clean data: associate entities to words in the sequence\n",
    "def clean_dataset(data):\n",
    "    \"\"\"\n",
    "    Processes a dataset of text and associated entity annotations to produce cleaned output.\n",
    "\n",
    "    This function iterates through text data and entity annotations, identifies word boundaries,\n",
    "    and associates entities with the corresponding words. If a word does not have an entity, it\n",
    "    is marked with \"Empty\". The cleaned data is returned in a pandas DataFrame format.\n",
    "\n",
    "    Arguments:\n",
    "        data (list): A list of tuples, where each tuple consists of:\n",
    "            - data[i][0] (str): A text string to be processed.\n",
    "            - data[i][1] (dict): A dictionary where keys are entity labels and values are lists \n",
    "              containing entity spans. Each span is a list [start_index, end_index, label].\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with a single column, \"sentences_cleaned\", containing a list\n",
    "        for each text entry. Each list has:\n",
    "            - Entity labels: Corresponding labels for words tagged with entities.\n",
    "            - \"Empty\": For words without associated entities.\n",
    "\n",
    "    Example Input:\n",
    "        data = [\n",
    "            (\"John Doe lives in Philadelphia.\", \n",
    "             {\"PERSON\": [[0, 8, \"John Doe\"]], \"LOCATION\": [[18, 31, \"Philadelphia\"]]}),\n",
    "            (\"Apple is a technology company.\", {})\n",
    "        ]\n",
    "\n",
    "    Example Output:\n",
    "        cleanedDF =\n",
    "            setences_cleaned\n",
    "        0   [\"PERSON\", \"Empty\", \"Empty\", \"Empty\", \"LOCATION\"]\n",
    "        1   [\"Empty\", \"Empty\", \"Empty\", \"Empty\", \"Empty\"]\n",
    "\n",
    "    Functionality:\n",
    "        1. Initializes an empty DataFrame to store cleaned data.\n",
    "        2. Iterates over the dataset using a progress bar for large datasets.\n",
    "        3. Breaks text into individual words and checks each word's span to associate entities.\n",
    "        4. Handles the last word in each text separately to ensure accurate annotation.\n",
    "        5. Appends processed annotations to the DataFrame.\n",
    "        6. Tracks the total number of processed words for monitoring purposes.\n",
    "\n",
    "    Notes:\n",
    "        - The \"Empty\" placeholder ensures every word has an annotation, even if no entities are present.\n",
    "        - Uses `tqdm` to display a progress bar for processing.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the cleaned word/entity association data for each text entry.\n",
    "    \"\"\"\n",
    "    cleanedDF = pd.DataFrame(columns=[\"sentences_cleaned\"])  # Initialize empty DataFrame\n",
    "    sum1 = 0  # Total number of words processed\n",
    "    \n",
    "    # Iterate over the dataset with progress bar\n",
    "    for ii in tqdm(range(len(data))):\n",
    "        start = 0  # Start index for the current word\n",
    "        emptyList = [\"Empty\"] * len(data[ii][0].split())  # Placeholder list for word annotations\n",
    "        numberOfWords = 0  # Count of processed words\n",
    "        lenOfString = len(data[ii][0])  # Length of the current text\n",
    "        strData = data[ii][0]  # Extract the text string\n",
    "        strDictData = data[ii][1]  # Extract entity annotations as dictionary\n",
    "        lastIndexOfSpace = strData.rfind(' ')  # Find the last space in the text\n",
    "        \n",
    "        # Iterate through characters in the text\n",
    "        for i in range(lenOfString):\n",
    "            # Handle word boundaries identified by spaces\n",
    "            if strData[i] == \" \" and strData[i + 1] != \" \":  # Detects the end of a word\n",
    "                for k, v in strDictData.items():  # Iterate through entity dictionary\n",
    "                    for j in range(len(v)):  # Check spans for overlapping entities\n",
    "                        entList = v[len(v) - j - 1]  # Process entity from the end of the list\n",
    "                        # Checks if current word (defined by start and i) overlaps with entity span (entList[0] to entList[1]).\n",
    "                        if start >= int(entList[0]) and i <= int(entList[1]):  \n",
    "                            emptyList[numberOfWords] = entList[2]  # Assign entity label\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                start = i + 1  # Update start index to the next word\n",
    "                numberOfWords += 1  # Increment word count\n",
    "            \n",
    "            # Handle the last word in the text\n",
    "            if i == lastIndexOfSpace:\n",
    "                for j in range(len(v)):\n",
    "                    entList = v[len(v) - j - 1]  # Process entity from the end of the list\n",
    "                    if lastIndexOfSpace >= int(entList[0]) and lenOfString <= int(entList[1]):\n",
    "                        emptyList[numberOfWords] = entList[2]  # Assign entity label\n",
    "                        numberOfWords += 1\n",
    "        \n",
    "        # Append processed data to the DataFrame\n",
    "        new_row = pd.DataFrame([[emptyList]], columns=cleanedDF.columns)\n",
    "        cleanedDF = pd.concat([cleanedDF, new_row], ignore_index=True)\n",
    "        sum1 = sum1 + numberOfWords  # Update total word count\n",
    "    \n",
    "    return cleanedDF"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:14.457717Z",
     "start_time": "2025-05-05T19:55:14.452823Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:18.272374Z",
     "start_time": "2025-05-05T19:55:17.993078Z"
    }
   },
   "source": [
    "cleaned_DF = clean_dataset(data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=220.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e854f290eb8435aa7f4d5d7707ddfce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:19.756811Z",
     "start_time": "2025-05-05T19:55:19.752112Z"
    }
   },
   "source": [
    "cleaned_DF.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   sentences_cleaned\n",
       "0  [Name, Name, Designation, Designation, Designa...\n",
       "1  [Name, Name, Empty, Empty, Empty, Empty, Empty...\n",
       "2  [Name, Name, Name, Empty, Empty, Empty, Empty,...\n",
       "3  [Name, Name, Designation, Designation, Designa...\n",
       "4  [Name, Name, Designation, Empty, Companies wor..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Name, Name, Empty, Empty, Empty, Empty, Empty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Name, Name, Name, Empty, Empty, Empty, Empty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Name, Name, Designation, Empty, Companies wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:24.710123Z",
     "start_time": "2025-05-05T19:55:24.701109Z"
    }
   },
   "source": [
    "# GENERATE UNIQUE TAGS AND PAD SEQUENCE\n",
    "# Extract unique tags from the 'sentences_cleaned' column of the DataFrame\n",
    "unique_tags = set(cleaned_DF['sentences_cleaned'].explode().unique())\n",
    "\n",
    "# Map each tag to a unique numerical ID\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "\n",
    "# Create a reverse mapping from IDs back to tags\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:27.085245Z",
     "start_time": "2025-05-05T19:55:27.082588Z"
    }
   },
   "source": [
    "unique_tags"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'College Name',\n",
       " 'Companies worked at',\n",
       " 'Degree',\n",
       " 'Designation',\n",
       " 'Email Address',\n",
       " 'Empty',\n",
       " 'Graduation Year',\n",
       " 'Location',\n",
       " 'Name',\n",
       " 'Skills',\n",
       " 'UNKNOWN',\n",
       " 'Years of Experience'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:31.002646Z",
     "start_time": "2025-05-05T19:55:30.999269Z"
    }
   },
   "source": [
    "# Find the maximum length across all entries in DF\n",
    "max_length = cleaned_DF['sentences_cleaned'].apply(len).max()\n",
    "print(\"Max sequence length:\", max_length)  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 2953\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:34.641607Z",
     "start_time": "2025-05-05T19:55:34.639079Z"
    }
   },
   "source": [
    "# Padding to ensure all input sequences have the same length. Sequences exceeding \n",
    "# the maximum length that is set will be truncated and shorter ones will be padded.\n",
    "MAX_LEN = 512  # initialize maximum length\n",
    "labels = cleaned_DF['sentences_cleaned'].values.tolist()\n",
    "print(labels[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Name', 'Designation', 'Designation', 'Designation', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Email Address', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Designation', 'Designation', 'Empty', 'Companies worked at', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Empty', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'College Name', 'College Name', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'College Name', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "tag2id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:42.344424Z",
     "start_time": "2025-05-05T19:55:42.340740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Graduation Year': 0,\n",
       " 'Skills': 1,\n",
       " 'Companies worked at': 2,\n",
       " 'College Name': 3,\n",
       " 'Years of Experience': 4,\n",
       " 'Name': 5,\n",
       " 'Location': 6,\n",
       " 'Degree': 7,\n",
       " 'Email Address': 8,\n",
       " 'Designation': 9,\n",
       " 'Empty': 10,\n",
       " 'UNKNOWN': 11}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Padding sequences\n",
    "# Create an empty list to store the converted sequences of tags\n",
    "converted_labels = []\n",
    "\n",
    "# Step 1: Iterate through each sequence of labels in 'labels'\n",
    "for label in labels:\n",
    "    current_sequence = []  # List to store the converted tags for the current sequence\n",
    "\n",
    "    # Step 2: Iterate through each label in the current sequence\n",
    "    for lab in label:\n",
    "        # Convert the label to its corresponding ID using the 'tag2id' dictionary\n",
    "        tag_id = tag2id.get(lab)\n",
    "        current_sequence.append(tag_id)  # Add the ID to the current sequence\n",
    "\n",
    "    # Append the fully converted sequence (list of IDs) to 'converted_labels'\n",
    "    converted_labels.append(current_sequence)\n",
    "\n",
    "# Step 3: Pad all the converted sequences to make them of uniform length\n",
    "tags = pad_sequences(\n",
    "    converted_labels,       # Nested list of tag IDs\n",
    "    maxlen=MAX_LEN,         # Maximum length for padding or truncation\n",
    "    value=tag2id[\"Empty\"],  # Value to use for padding\n",
    "    padding=\"post\",         # Add padding at the end of the sequence\n",
    "    dtype=\"long\",           # Data type of the resulting array\n",
    "    truncating=\"post\"       # Truncate sequences that are too long from the end\n",
    ")\n",
    "\n",
    "tags"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:51.585557Z",
     "start_time": "2025-05-05T19:55:51.573352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  5,  9, ..., 10, 10, 10],\n",
       "       [ 5,  5, 10, ..., 10, 10, 10],\n",
       "       [ 5,  5,  5, ..., 10,  1, 10],\n",
       "       ...,\n",
       "       [ 5,  5,  9, ..., 10, 10, 10],\n",
       "       [ 5,  5,  9, ..., 10, 10, 10],\n",
       "       [ 5,  5,  9, ..., 10, 10, 10]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:55:57.517463Z",
     "start_time": "2025-05-05T19:55:57.510954Z"
    }
   },
   "source": [
    "# TOKENIZE AND ALIGN LABELS WITH LIBRARY\n",
    "def tokenize_and_align_labels(tokenizer, examples, tags, max_tokens=512, label_all_tokens=True):\n",
    "    \"\"\"\n",
    "    Tokenizes input text and aligns word-level labels with tokenized subwords.\n",
    "\n",
    "    This function processes raw text data using a tokenizer, ensuring the resulting \n",
    "    subword tokens are correctly aligned with their corresponding word-level labels (tags). \n",
    "    It handles special tokens, subword alignment, and padding/truncation based on predefined parameters.\n",
    "\n",
    "    Arguments:\n",
    "        tokenizer: \n",
    "            A tokenizer object (e.g., Hugging Face tokenizer) used to process raw text into tokens.\n",
    "        examples (list of str): \n",
    "            A list of raw text sequences to be tokenized.\n",
    "        tags (list of list): \n",
    "            A list of word-level label sequences corresponding to `examples`.\n",
    "            Each element is a list of labels for the words in the text.\n",
    "        max_tokens (int): Maximum length of tokenized input. Default = 512\n",
    "        label_all_tokens (bool): Boolean to label all tokens. Default is True\n",
    "        \n",
    "    Returns:\n",
    "        dict: \n",
    "            A dictionary containing tokenized input data and aligned labels:\n",
    "            - 'input_ids': Token IDs for the sequences.\n",
    "            - 'attention_mask': Attention masks for distinguishing padding from actual tokens.\n",
    "            - 'labels': Aligned label sequences, where special tokens are assigned a label of -100.\n",
    "\n",
    "    Function Steps:\n",
    "        1. Tokenize input text using the provided tokenizer.\n",
    "        2. Retrieve word IDs to map tokens back to their original words.\n",
    "        3. Align labels with tokens:\n",
    "           - Assign `-100` to special tokens (e.g., [CLS], [SEP]) to exclude them from the loss computation.\n",
    "           - Assign word-level labels to the first subword of each tokenized word.\n",
    "           - Depending on the `label_all_tokens` flag, assign labels to all subwords or use `-100` for non-first subwords.\n",
    "        4. Return tokenized inputs along with aligned labels.\n",
    "\n",
    "    Example:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        examples = [\"John lives in New York.\"]\n",
    "        tags = [[2, 0, 0, 1]]  # Labels for each word in the sequence\n",
    "        tokenized_data = tokenize_and_align_labels(tokenizer, examples, tags)\n",
    "        \n",
    "        Output:\n",
    "        {\n",
    "            'input_ids': [101, ..., 102],          # Token IDs for the text\n",
    "            'attention_mask': [1, ..., 0],         # Mask to indicate valid tokens\n",
    "            'labels': [-100, 0, ..., -100]         # Aligned label sequence\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize input text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples, \n",
    "        truncation=True,\n",
    "        is_split_into_words=False,  # Assumes each input is a raw text sequence, not already split into words.\n",
    "        padding='max_length',  # Adds padding to ensure all sequences have the same length (e.g., 512 tokens).\n",
    "        max_length=max_tokens  # Maximum length for tokenized sequences.\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(tags):\n",
    "        # Align word-level labels (tags) with tokenized subwords\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. Assign -100 to exclude them from loss computation.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Assign label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # Assign label for subwords based on label_all_tokens flag.\n",
    "            else:\n",
    "                # Handling subword token: Subwords of the same word (i.e., word_idx == previous_word_idx)\n",
    "                # This applies to tokens that are part of a word but are not the first token (e.g., subwords\n",
    "                # produced by tokenizers like BERT).\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Add aligned labels to the tokenized input dictionary\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:56:03.098640Z",
     "start_time": "2025-05-05T19:56:02.739303Z"
    }
   },
   "source": [
    "# Load the pretrained tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize and align labels\n",
    "tk_inputs = tokenize_and_align_labels(tokenizer, df_data['content'].values.tolist(), tags, max_tokens=MAX_LEN)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((tk_inputs['input_ids'],\n",
    "                                                    tk_inputs['labels']))\n",
    "print(train_dataset.take(1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=(TensorSpec(shape=(512,), dtype=tf.int32, name=None), TensorSpec(shape=(512,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset.cardinality().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:56:09.114631Z",
     "start_time": "2025-05-05T19:56:09.111887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "for example in train_dataset.take(1):  # Retrieve one example\n",
    "    input_ids, labels = example\n",
    "    print(\"Input IDs:\", input_ids.numpy())\n",
    "    print(\"Labels:\", labels.numpy())\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:56:10.842198Z",
     "start_time": "2025-05-05T19:56:10.826069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [  101 11113 24158  5369  2243  1046  3270  4646  2458  5482  1011  9669\n",
      "  5397  8191 14129  1010 12092  1011 10373  2033  2006  5262  1024  5262\n",
      "  1012  4012  1013  1054  1013 11113 24158  5369  2243  1011  1046  3270\n",
      "  1013  2184  2063  2581  2050  2620 27421  2581 16703  9818 23777  2050\n",
      "  1528  2000  2147  2005  2019  3029  2029  3640  2033  1996  4495  2000\n",
      "  5335  2026  4813  1998  3716  2005  2026  3265  1998  2194  1005  1055\n",
      "  3930  1999  2190  2825  3971  1012  5627  2000 20102  2000  1024 14022\n",
      "  1010 12092  2147  3325  4646  2458  5482  9669  5397  1011  2281  2418\n",
      "  2000  2556  2535  1024  2747  2551  2006 11834  1011 28516  1012  4975\n",
      "  2067 10497 14721  7243 15794 10861  5134  2005  1996 28516  2029  2097\n",
      "  2022 13330  2241  2006  2445  7953  1012  2036  1010  2731  1996 28516\n",
      "  2005  2367  2825 14395 26755  1006  2119  3893  1998  4997  1007  1010\n",
      "  2029  2097  2022  2445  2004  7953  2011  1996  5310  1012  2495  1038\n",
      "  1012  1041  1999  2592  2671  1998  3330  1038  1012  1058  1012  1038\n",
      "  2267  1997  3330  1998  2974  1011  9594  3669  1010 12092  2257  2286\n",
      "  2000  2238  2418  5940  1999  5597  3536 16765  2715  2082  2258  2249\n",
      "  2000  2233  2286  6049  6358 13626  8717  6819 25838 22923  2258  2541\n",
      "  2000  2233  2249  4813  1039  1006  2625  2084  1015  2095  1007  1010\n",
      "  7809  1006  2625  2084  1015  2095  1007  1010  7809  2968  1006  2625\n",
      "  2084  1015  2095  1007  1010  7809  2968  2291  1006  2625  2084  1015\n",
      "  2095  1007  1010  9262  1006  2625  2084  1015  2095  1007  3176  2592\n",
      "  4087  4813 16770  1024  1013  1013  7479  1012  5262  1012  4012  1013\n",
      "  1054  1013 11113 24158  5369  2243  1011  1046  3270  1013  2184  2063\n",
      "  2581  2050  2620 27421  2581 16703  9818 23777  2050  1029  2003  3593\n",
      "  1027 10151  1011  8816  1004 20912  2860  1027  8816  1011  2327  1004\n",
      "  2522  1027  1999  1528  4730  2653  1024  1039  1010  1039  1009  1009\n",
      "  1010  9262  1528 14721  7243 15794  1528  4274  1997  2477  1528  3698\n",
      "  4083  1528  7809  2968  2291  1528  3274  6125  1528  4082  2291  2499\n",
      "  2006  1024 11603  1010  3645  1010  6097  2512  1011  4087  4813  1528\n",
      "  7481  1998  2524  1011  2551  1528 23691  1998 12379  2000  2367  8146\n",
      "  1528 13205  1998  5475  1528  2136  1011  2447   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "Labels: [-100    5    5    5    5    5    5    9    9    9   10   10   10   10\n",
      "   10   10   10   10   10   10    8   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10    9    9   10    2   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10    9    9    9    9    9   10    3    3    3    3    3    3    3\n",
      "   10   10   10   10   10   10   10   10   10   10   10    3    3   10\n",
      "   10   10   10   10   10   10    3   10   10   10   10   10   10   10\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1   10   10   10   10   10   10   10   10    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10   10 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:56:19.096430Z",
     "start_time": "2025-05-05T19:56:17.948404Z"
    }
   },
   "source": [
    "# OPTIMIZATION\n",
    "# Optimizing DistilBERT model to match the tokenizer to preprocess your data.\n",
    "# Model\n",
    "model = TFDistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(unique_tags))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:56:23.083721Z",
     "start_time": "2025-05-05T19:56:23.076028Z"
    }
   },
   "source": [
    "# Compile model\n",
    "from tf_keras.optimizers.legacy import Adam\n",
    "optimizer = Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy']) # can also use any keras loss fn"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = int(train_dataset.cardinality().numpy()) \n",
    "\n",
    "# Properly process the dataset\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)  # Shuffle the dataset\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)     # Batch the dataset\n",
    "train_dataset = train_dataset.prefetch(AUTOTUNE)    # Add AUTOTUNE for efficient data loading\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T19:57:56.877685Z",
     "start_time": "2025-05-05T19:56:35.411619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 81s 1s/step - loss: 0.8678 - accuracy: 0.7116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x30598eba0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the callback to save the best model based on training loss\n",
    "from tf_keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "check_point = ModelCheckpoint(\n",
    "    filepath='best_model.h5',  # Path to save the model\n",
    "    monitor='loss',            # Monitor the training loss instead of validation loss\n",
    "    save_best_only=True,       # Save only when the monitored metric improves\n",
    "    save_weights_only=True,    # Save weights only\n",
    "    mode='min',                # Minimize the loss (the lower, the better)\n",
    "    verbose=1                  # Print progress messages\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Model training with the callback\n",
    "model.fit(train_dataset, epochs=10, callbacks=[check_point, early_stopping])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-05T20:30:09.077918Z",
     "start_time": "2025-05-05T20:16:43.623963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.7947\n",
      "Epoch 1: loss improved from inf to 0.21141, saving model to best_model.h5\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.2114 - accuracy: 0.7947\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.7968\n",
      "Epoch 2: loss improved from 0.21141 to 0.19997, saving model to best_model.h5\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.2000 - accuracy: 0.7968\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.7974\n",
      "Epoch 3: loss improved from 0.19997 to 0.19578, saving model to best_model.h5\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.1958 - accuracy: 0.7974\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.7981\n",
      "Epoch 4: loss improved from 0.19578 to 0.19363, saving model to best_model.h5\n",
      "55/55 [==============================] - 86s 2s/step - loss: 0.1936 - accuracy: 0.7981\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.7962\n",
      "Epoch 5: loss did not improve from 0.19363\n",
      "55/55 [==============================] - 78s 1s/step - loss: 0.1943 - accuracy: 0.7962\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.7985\n",
      "Epoch 6: loss improved from 0.19363 to 0.18754, saving model to best_model.h5\n",
      "55/55 [==============================] - 77s 1s/step - loss: 0.1875 - accuracy: 0.7985\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.7994\n",
      "Epoch 7: loss improved from 0.18754 to 0.17871, saving model to best_model.h5\n",
      "55/55 [==============================] - 83s 2s/step - loss: 0.1787 - accuracy: 0.7994\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.8007\n",
      "Epoch 8: loss improved from 0.17871 to 0.17280, saving model to best_model.h5\n",
      "55/55 [==============================] - 82s 1s/step - loss: 0.1728 - accuracy: 0.8007\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.8003\n",
      "Epoch 9: loss did not improve from 0.17280\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.1741 - accuracy: 0.8003\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.8002\n",
      "Epoch 10: loss did not improve from 0.17280\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.1733 - accuracy: 0.8002\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x326596690>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training takes time. Let's pause and assume that the model has been fully trained...\n",
    "Let's write codes to make predictions on a trained model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# PREDICTIONS\n",
    "text = \"Moe Ali, 5y Application Developer at Apple. Saurashtra University graduate. Email: moe-ali@me.com. Location: Philadelphia. Skills: problem solving (Less than 1 year), project lifecycle (Less than 1 year), project manager (Less than 1 year), technical assistance (Less than 1 year).\"\n",
    "text_inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, is_split_into_words=False, padding=\"max_length\", max_length=MAX_LEN)\n",
    "input_ids = text_inputs[\"input_ids\"]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:30:30.375782Z",
     "start_time": "2025-05-05T20:30:30.371751Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "text_logits = model(text_inputs).logits\n",
    "text_prediction = np.argmax(text_logits, axis=-1)\n",
    "\n",
    "# Convert token IDs to readable words\n",
    "tokens = tokenizer.convert_ids_to_tokens(text_inputs[\"input_ids\"][0])\n",
    "print(text_prediction)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:30:40.062073Z",
     "start_time": "2025-05-05T20:30:39.712053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  5  5  5 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10  1 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10  7 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10  7 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10  1 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10  5 10  5 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10  5 10  5 10 10 10 10 10 10 10 10 10 10 10 10 10  3 10 10\n",
      "   7 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10  7 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  10 10 10 10 10 10 10 10]]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:30:50.523758Z",
     "start_time": "2025-05-05T20:30:50.520443Z"
    }
   },
   "cell_type": "code",
   "source": "text_ner_extract = extract_named_entities(text_prediction, id2tag, tokens, output_filename=\"ner_extracted_data.txt\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered NER extracted data saved to ner_extracted_data.txt\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:30:52.066578Z",
     "start_time": "2025-05-05T20:30:51.799971Z"
    }
   },
   "source": [
    "model(text_inputs)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFTokenClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 512, 12), dtype=float32, numpy=\n",
       "array([[[-0.4998773 ,  0.33691654, -0.4542086 , ...,  0.14848809,\n",
       "          0.7091168 , -0.28086126],\n",
       "        [-0.7556005 , -0.189406  , -0.41208318, ...,  0.9605484 ,\n",
       "          0.01034614, -0.776591  ],\n",
       "        [-0.67618185, -0.5233857 , -0.4622652 , ...,  1.0638667 ,\n",
       "         -0.07557663, -0.6665334 ],\n",
       "        ...,\n",
       "        [-0.39389208,  1.3292242 , -0.2599512 , ..., -0.37116396,\n",
       "          4.3841424 , -0.443672  ],\n",
       "        [-0.29335445,  1.659098  , -0.39312083, ..., -0.7903069 ,\n",
       "          4.6886377 , -0.23070002],\n",
       "        [-0.12853125,  1.822825  , -0.5919449 , ..., -0.7053789 ,\n",
       "          4.423382  , -0.28691804]]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:30:54.102101Z",
     "start_time": "2025-05-05T20:30:54.077217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred_labels, true_labels = [], []\n",
    "for i in range(len(tk_inputs['labels'])):\n",
    "    current_true_labels = []\n",
    "    for true_index in tk_inputs['labels'][i]:\n",
    "        current_true_labels.append(id2tag.get(true_index, \"Empty\"))\n",
    "    true_labels.append(current_true_labels)\n",
    "np.array(true_labels).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 512)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:31:25.273178Z",
     "start_time": "2025-05-05T20:30:54.903016Z"
    }
   },
   "source": [
    "output = model.predict(train_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 30s 552ms/step\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:31:45.704048Z",
     "start_time": "2025-05-05T20:31:45.700926Z"
    }
   },
   "source": [
    "m = len(tk_inputs['labels'])  # Number of samples\n",
    "num_classes = len(unique_tags)  # Number of classes\n",
    "predictions = np.argmax(output['logits'], axis=-1)"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:31:46.421695Z",
     "start_time": "2025-05-05T20:31:46.405459Z"
    }
   },
   "source": [
    "pred_labels = [[id2tag.get(index, \"Empty\") for index in predictions[i]] for i in range(len(predictions))]"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T20:31:48.756434Z",
     "start_time": "2025-05-05T20:31:46.917261Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from seqeval\n",
    "warnings.filterwarnings(\"ignore\", message=\".*seems not to be NE tag.*\")\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, zero_division=0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            NKNOWN       0.00      0.00      0.00         1\n",
      "               ame       0.19      0.19      0.19       220\n",
      "ears of Experience       0.00      0.00      0.00        37\n",
      "             egree       0.00      0.00      0.00       144\n",
      "        esignation       0.02      0.01      0.02       430\n",
      "             kills       0.05      0.06      0.05      4704\n",
      "      mail Address       0.00      0.00      0.00        76\n",
      "              mpty       0.93      0.94      0.93    103155\n",
      "           ocation       0.00      0.00      0.00        73\n",
      "       ollege Name       0.00      0.00      0.00       214\n",
      "ompanies worked at       0.00      0.00      0.00       470\n",
      "    raduation Year       0.00      0.00      0.00        58\n",
      "\n",
      "         micro avg       0.88      0.89      0.88    109582\n",
      "         macro avg       0.10      0.10      0.10    109582\n",
      "      weighted avg       0.87      0.89      0.88    109582\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Note: This completes the Resume Named-entity Recognition (NER) to help process resumes. Additional training is needed before feeding your input into the transformer model."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
